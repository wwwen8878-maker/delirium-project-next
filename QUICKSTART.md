# 🚀 快速启动指南

## 🎯 完成情况

✅ **开源LLM集成完成** - 支持Ollama本地部署  
✅ **UI风格去AI化** - 医疗专业化设计  
✅ **目标用户明确** - 面向术前患者和家属  
✅ **智能助手升级** - 全新对话体验  
✅ **依赖包已安装** - 153个新包，0漏洞  

---

## 💻 立即运行（不需要LLM）

如果你暂时不想配置LLM，可以直接运行查看界面：

```bash
npm run dev
```

访问 http://localhost:3000

**说明**：智能助手会使用兜底回复机制（基于关键词匹配），虽然不如真实LLM智能，但仍能提供有用的信息。

---

## 🤖 配置LLM（推荐体验）

### 方式1：本地Ollama（5分钟配置）

#### 步骤1：安装Ollama
```bash
# Windows/Mac/Linux
访问 https://ollama.com 下载安装
```

#### 步骤2：下载模型
```bash
# 轻量级模型（推荐，4GB内存）
ollama pull llama3.2:3b

# 或中文优化模型（8GB内存）
ollama pull qwen2.5:7b
```

#### 步骤3：启动服务
```bash
ollama serve
# 服务运行在 http://localhost:11434
```

#### 步骤4：创建配置文件
在项目根目录创建 `.env.local`：

```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

#### 步骤5：重启开发服务器
```bash
npm run dev
```

**完成！** 🎉 点击右下角智能助手图标，开始对话！

---

### 方式2：云端API（无需本地安装）

如果不想本地安装Ollama，可以使用云端开源模型API：

#### OpenRouter（推荐，有免费额度）
注册：https://openrouter.ai

`.env.local`：
```env
OPENROUTER_API_KEY=sk-or-v1-xxxxx
OPENROUTER_MODEL=meta-llama/llama-3.2-3b-instruct:free
```

#### Together AI
注册：https://together.ai

`.env.local`：
```env
TOGETHER_API_KEY=xxxxx
TOGETHER_MODEL=meta-llama/Llama-3.2-3B-Instruct-Turbo
```

---

## 🧪 测试智能助手

运行后，点击右下角**蓝紫色圆形按钮**，尝试提问：

- "谵妄是什么？"
- "如何预防谵妄？"
- "家属如何参与？"
- "术后有哪些症状？"

---

## 📱 主要功能

### 1. 首页
- ✅ 明确定位：**为手术患者及家属服务**
- ✅ 清晰说明：术前/术后/家庭护理三大场景
- ✅ 适用人群：65岁以上患者、家属陪护者、医护健康教育

### 2. 风险评估
- ✅ 去AI化文案（"科学风险评估"而非"AI智能评估"）
- ✅ 详细问卷（年龄、手术类型、认知功能等）
- ✅ 个性化报告和预防建议

### 3. 智能助手（右下角）
- ✅ 医疗场景专用对话
- ✅ 快捷问题引导
- ✅ 上下文理解
- ✅ 通俗易懂的回答
- ✅ 医疗免责说明

### 4. 其他页面
- 📖 术前科普指南
- 🔍 术后谵妄筛查
- 📋 预防计划管理
- 📊 健康日记记录

---

## 🎨 UI风格对比

### 更新前
- ❌ "AI智能助手正在学习中"
- ❌ "智能化辅助工具"
- ❌ "专业医疗服务平台"
- ❌ 科技感强，AI感明显

### 更新后
- ✅ "健康教育助手 - 谵妄预防知识"
- ✅ "通俗易懂讲解"
- ✅ "为手术患者及家属服务"
- ✅ 医疗专业化，人性化

---

## 🔧 故障排查

### Q: 智能助手不回复？
**A**: 检查：
1. Ollama服务是否启动：`ollama serve`
2. 模型是否下载：`ollama list`
3. `.env.local`配置是否正确
4. 查看浏览器控制台错误信息

### Q: 回复内容不专业？
**A**: 
- 使用更大的模型（如`qwen2.5:7b`）
- 或调整`src/lib/llm-service.ts`中的系统提示词

### Q: 依赖安装失败？
**A**: 
```bash
# 清理缓存重试
npm cache clean --force
npm install --legacy-peer-deps
```

---

## 📚 文档索引

- **SETUP_LLM.md** - LLM详细配置指南
- **README_UPDATES.md** - 本次更新完整说明
- **QUICKSTART.md** - 本文档

---

## 🎉 主要亮点

### 1. 完全开源
- 使用Meta Llama、Qwen等开源模型
- 可本地运行，数据不出本地
- 无需商业API，成本低

### 2. 医疗专业
- 针对谵妄场景优化的提示词
- 循证医学知识库
- 通俗易懂的语言表达
- 明确的医疗免责

### 3. 用户友好
- 清晰的目标人群（患者+家属）
- 具体的使用场景说明
- 快捷问题引导
- 移动端友好设计

### 4. 隐私安全
- 支持本地LLM部署
- 评估数据存储在浏览器本地
- 不上传个人健康信息

---

## 💡 推荐模型

| 模型 | 内存 | 中文 | 速度 | 推荐场景 |
|------|------|------|------|----------|
| llama3.2:3b | 4GB | ⭐⭐⭐ | ⚡⚡⚡ | 快速体验 |
| qwen2.5:7b | 8GB | ⭐⭐⭐⭐⭐ | ⚡⚡ | 中文优先 |
| meditron:7b | 8GB | ⭐⭐⭐ | ⚡⚡ | 医疗专业 |

---

## 🚀 立即开始

```bash
# 1. 安装依赖（已完成）
npm install

# 2. 运行开发服务器
npm run dev

# 3. 访问网站
open http://localhost:3000

# 4. (可选) 配置Ollama体验完整功能
# 参考上面的"配置LLM"部分
```

---

**祝你使用愉快！** 🎉

如有问题，请查看详细文档或提交Issue。

