# ğŸŒ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

## é—®é¢˜è¯´æ˜

å½“å‰é…ç½®ä½¿ç”¨ `localhost:11434` çš„Ollamaï¼Œ**ä»…é€‚ç”¨äºæœ¬åœ°å¼€å‘**ã€‚

âŒ **é—®é¢˜**ï¼šç”¨æˆ·è®¿é—®éƒ¨ç½²åçš„ç½‘ç«™æ—¶ï¼Œæ— æ³•è¿æ¥åˆ°ä½ æœ¬åœ°çš„OllamaæœåŠ¡ã€‚

âœ… **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨äº‘ç«¯LLM APIæœåŠ¡ã€‚

---

## ğŸš€ æ¨èæ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### æ–¹æ¡ˆ1ï¼šGroq APIï¼ˆæ¨è â­â­â­â­â­ï¼‰

**ä¼˜åŠ¿**ï¼š
- âš¡ **è¶…å¿«é€Ÿåº¦**ï¼ˆæ¯”å…¶ä»–å¿«10å€ï¼‰
- ğŸ†“ **å…è´¹é¢åº¦å……è¶³**ï¼ˆæ¯å¤©å¯å¤„ç†æ•°åƒæ¬¡è¯·æ±‚ï¼‰
- ğŸ¤– **å¼€æºæ¨¡å‹**ï¼ˆLlama 3.2, Mixtralç­‰ï¼‰
- ğŸŒ **å®˜æ–¹ç¨³å®š**

**é…ç½®æ­¥éª¤**ï¼š

1. æ³¨å†Œè´¦å·ï¼šhttps://console.groq.com
2. åˆ›å»ºAPI Key
3. æ·»åŠ ç¯å¢ƒå˜é‡ï¼š

```env
# .env.local (æœ¬åœ°å¼€å‘)
GROQ_API_KEY=gsk_xxxxxxxxxxxx
GROQ_MODEL=llama-3.2-3b-preview
```

4. ç”Ÿäº§ç¯å¢ƒåœ¨Vercel/Netlifyç­‰å¹³å°è®¾ç½®ç¯å¢ƒå˜é‡

---

### æ–¹æ¡ˆ2ï¼šOpenRouterï¼ˆæ¨è â­â­â­â­ï¼‰

**ä¼˜åŠ¿**ï¼š
- ğŸ¯ **å¤šæ¨¡å‹èšåˆ**ï¼ˆä¸€ä¸ªAPIè®¿é—®50+æ¨¡å‹ï¼‰
- ğŸ†“ **æœ‰å…è´¹æ¨¡å‹**ï¼ˆå¦‚Llama 3.2å…è´¹ç‰ˆï¼‰
- ğŸ’° **æŒ‰éœ€ä»˜è´¹**ï¼ˆå…¶ä»–æ¨¡å‹æŒ‰å®é™…ä½¿ç”¨è®¡è´¹ï¼‰

**é…ç½®**ï¼š
```env
OPENROUTER_API_KEY=sk-or-v1-xxxxx
OPENROUTER_MODEL=meta-llama/llama-3.2-3b-instruct:free
```

æ³¨å†Œï¼šhttps://openrouter.ai

---

### æ–¹æ¡ˆ3ï¼šTogether AIï¼ˆæ¨è â­â­â­â­ï¼‰

**ä¼˜åŠ¿**ï¼š
- ğŸš€ **ä¸“æ³¨å¼€æºæ¨¡å‹**
- ğŸ’° **ä»·æ ¼ä¾¿å®œ**ï¼ˆ$0.2 / ç™¾ä¸‡tokensï¼‰
- ğŸ **æ–°ç”¨æˆ·$25å…è´¹é¢åº¦**

**é…ç½®**ï¼š
```env
TOGETHER_API_KEY=xxxxx
TOGETHER_MODEL=meta-llama/Llama-3.2-3B-Instruct-Turbo
```

æ³¨å†Œï¼šhttps://together.ai

---

### æ–¹æ¡ˆ4ï¼šCloudflare Workers AIï¼ˆæ€§ä»·æ¯”é«˜ â­â­â­â­ï¼‰

**ä¼˜åŠ¿**ï¼š
- ğŸ’° **æä½ä»·æ ¼**ï¼ˆæ¯å¤©10,000æ¬¡å…è´¹è¯·æ±‚ï¼‰
- âš¡ **è¾¹ç¼˜è®¡ç®—**ï¼ˆå…¨çƒä½å»¶è¿Ÿï¼‰
- ğŸ”’ **CloudflareåŸºç¡€è®¾æ–½**

éœ€è¦ä¿®æ”¹ä»£ç é›†æˆï¼Œè¾ƒå¤æ‚ã€‚

---

## ğŸ› ï¸ å®æ–½æ­¥éª¤

### æ­¥éª¤1ï¼šæ›´æ–° LLM æœåŠ¡ä»£ç 

ä¿®æ”¹ `src/lib/llm-service.ts`ï¼Œæ·»åŠ Groqæ”¯æŒï¼š

```typescript
/**
 * è°ƒç”¨Groq APIï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
 */
async function callGroq(messages: ChatMessage[]): Promise<LLMResponse> {
  const apiKey = process.env.GROQ_API_KEY;
  const model = process.env.GROQ_MODEL || 'llama-3.2-3b-preview';

  if (!apiKey) {
    return {
      content: '',
      error: 'æœªé…ç½®Groq APIå¯†é’¥'
    };
  }

  try {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages,
        temperature: 0.7,
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      throw new Error(`Groq API error: ${response.status}`);
    }

    const data = await response.json();
    return { content: data.choices[0].message.content };
  } catch (error) {
    console.error('Groqè°ƒç”¨å¤±è´¥:', error);
    return {
      content: '',
      error: 'GroqæœåŠ¡æš‚æ—¶ä¸å¯ç”¨'
    };
  }
}

/**
 * ä¸»èŠå¤©å‡½æ•° - æ ¹æ®ç¯å¢ƒè‡ªåŠ¨é€‰æ‹©LLM
 */
export async function chat(userMessage: string, conversationHistory: ChatMessage[] = []): Promise<LLMResponse> {
  const messages: ChatMessage[] = [
    { role: 'system', content: SYSTEM_PROMPT },
    ...conversationHistory,
    { role: 'user', content: userMessage }
  ];

  // ä¼˜å…ˆçº§1ï¼šGroqï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰
  if (process.env.GROQ_API_KEY) {
    const result = await callGroq(messages);
    if (!result.error) {
      return result;
    }
  }

  // ä¼˜å…ˆçº§2ï¼šOpenRouter
  if (process.env.OPENROUTER_API_KEY) {
    const result = await callOpenRouter(messages);
    if (!result.error) {
      return result;
    }
  }

  // ä¼˜å…ˆçº§3ï¼šTogether AI
  if (process.env.TOGETHER_API_KEY) {
    const result = await callTogether(messages);
    if (!result.error) {
      return result;
    }
  }

  // ä¼˜å…ˆçº§4ï¼šæœ¬åœ°Ollamaï¼ˆä»…å¼€å‘ç¯å¢ƒï¼‰
  if (process.env.OLLAMA_BASE_URL) {
    const result = await callOllama(messages);
    if (!result.error) {
      return result;
    }
  }

  // å…œåº•å›å¤
  return {
    content: getFallbackResponse(userMessage),
    error: 'æ™ºèƒ½åŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¿™æ˜¯é¢„è®¾å›å¤'
  };
}
```

---

### æ­¥éª¤2ï¼šç¯å¢ƒå˜é‡é…ç½®

#### æœ¬åœ°å¼€å‘ (`.env.local`)
```env
# å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨æœ¬åœ°Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# æˆ–ä½¿ç”¨äº‘ç«¯APIæµ‹è¯•
# GROQ_API_KEY=gsk_xxxxx
# GROQ_MODEL=llama-3.2-3b-preview
```

#### ç”Ÿäº§ç¯å¢ƒï¼ˆVercel/Netlifyï¼‰
åœ¨éƒ¨ç½²å¹³å°çš„ç¯å¢ƒå˜é‡è®¾ç½®ä¸­æ·»åŠ ï¼š

```env
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx
GROQ_MODEL=llama-3.2-3b-preview
```

**é‡è¦**ï¼šä¸è¦æŠŠAPI Keyæäº¤åˆ°Gitä»“åº“ï¼

---

## ğŸ“‹ å„å¹³å°éƒ¨ç½²æŒ‡å—

### Vercel éƒ¨ç½²

1. **è¿æ¥GitHubä»“åº“**
   ```bash
   # æ¨é€ä»£ç åˆ°GitHub
   git add .
   git commit -m "æ·»åŠ äº‘ç«¯LLMæ”¯æŒ"
   git push
   ```

2. **å¯¼å…¥é¡¹ç›®åˆ°Vercel**
   - è®¿é—® https://vercel.com
   - ç‚¹å‡» "Add New Project"
   - é€‰æ‹©ä½ çš„GitHubä»“åº“

3. **é…ç½®ç¯å¢ƒå˜é‡**
   åœ¨ Vercel Dashboard â†’ Settings â†’ Environment Variables æ·»åŠ ï¼š
   ```
   GROQ_API_KEY = gsk_xxxxx
   GROQ_MODEL = llama-3.2-3b-preview
   ```

4. **éƒ¨ç½²**
   - ç‚¹å‡» "Deploy"
   - å‡ åˆ†é’Ÿåå³å¯è®¿é—®

---

### Netlify éƒ¨ç½²

1. **è¿æ¥ä»“åº“**
   - è®¿é—® https://netlify.com
   - ç‚¹å‡» "Add new site" â†’ "Import from Git"

2. **æ„å»ºè®¾ç½®**
   ```
   Build command: npm run build
   Publish directory: out æˆ– .next
   ```

3. **ç¯å¢ƒå˜é‡**
   åœ¨ Site settings â†’ Environment variables æ·»åŠ ï¼š
   ```
   GROQ_API_KEY = gsk_xxxxx
   GROQ_MODEL = llama-3.2-3b-preview
   ```

---

### è‡ªæœ‰æœåŠ¡å™¨éƒ¨ç½²

å¦‚æœä½ æœ‰è‡ªå·±çš„æœåŠ¡å™¨ï¼ˆå¦‚é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ï¼‰ï¼š

#### æ–¹æ¡ˆAï¼šéƒ¨ç½²Ollamaåˆ°äº‘æœåŠ¡å™¨

```bash
# 1. SSHè¿æ¥æœåŠ¡å™¨
ssh user@your-server.com

# 2. å®‰è£…Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 3. ä¸‹è½½æ¨¡å‹
ollama pull llama3.2:3b

# 4. å¯åŠ¨æœåŠ¡ï¼ˆç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£ï¼‰
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# 5. é…ç½®Nginxåå‘ä»£ç†
# /etc/nginx/sites-available/ollama
server {
    listen 443 ssl;
    server_name ollama.yourdomain.com;
    
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

ç„¶ååœ¨é¡¹ç›®ç¯å¢ƒå˜é‡ä¸­ï¼š
```env
OLLAMA_BASE_URL=https://ollama.yourdomain.com
OLLAMA_MODEL=llama3.2:3b
```

#### æ–¹æ¡ˆBï¼šä½¿ç”¨äº‘ç«¯API
ç›´æ¥é…ç½®Groqç­‰äº‘ç«¯APIï¼Œæ— éœ€ç»´æŠ¤æœåŠ¡å™¨ã€‚

---

## ğŸ’° æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | å…è´¹é¢åº¦ | ä»˜è´¹ä»·æ ¼ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|----------|
| **Groq** | æ¯å¤©å¤§é‡å…è´¹ | - | ğŸ† **å°ä¸­å‹é¡¹ç›®é¦–é€‰** |
| **OpenRouter** | éƒ¨åˆ†æ¨¡å‹å…è´¹ | æŒ‰æ¨¡å‹ä¸åŒ | éœ€è¦å¤šæ¨¡å‹åˆ‡æ¢ |
| **Together AI** | $25æ–°ç”¨æˆ· | $0.2/M tokens | ä¸­å‹é¡¹ç›® |
| **è‡ªå»ºOllama** | æ— é™åˆ¶ | æœåŠ¡å™¨è´¹ç”¨ | å¤§å‹é¡¹ç›®/éšç§è¦æ±‚é«˜ |

### æµé‡é¢„ä¼°

å‡è®¾ï¼š
- å¹³å‡æ¯æ¬¡å¯¹è¯ï¼š200 tokensï¼ˆçº¦150å­—ï¼‰
- æ¯å¤©100ä¸ªç”¨æˆ·ï¼Œæ¯äºº3æ¬¡å¯¹è¯
- æ¯æœˆæ€»tokensï¼š100äºº Ã— 3å¯¹è¯ Ã— 200 tokens Ã— 30å¤© = 1.8M tokens

**æˆæœ¬**ï¼š
- Groqï¼šå…è´¹ âœ…
- Together AIï¼š$0.36/æœˆ
- è‡ªå»ºæœåŠ¡å™¨ï¼šÂ¥50-100/æœˆï¼ˆå«æœåŠ¡å™¨ï¼‰

---

## ğŸ”’ å®‰å…¨å»ºè®®

### 1. API Key ä¿æŠ¤
```typescript
// âŒ é”™è¯¯ï¼šç›´æ¥æš´éœ²åœ¨å‰ç«¯
const apiKey = 'gsk_xxxxx'; // æ°¸è¿œä¸è¦è¿™æ ·åšï¼

// âœ… æ­£ç¡®ï¼šä½¿ç”¨æœåŠ¡å™¨ç«¯APIè·¯ç”±
// src/app/api/chat/route.ts
const apiKey = process.env.GROQ_API_KEY; // æœåŠ¡å™¨ç«¯è¯»å–
```

### 2. é€Ÿç‡é™åˆ¶
```typescript
// src/app/api/chat/route.ts
import { rateLimit } from '@/lib/rate-limit';

export async function POST(request: NextRequest) {
  // é™åˆ¶æ¯ä¸ªIPæ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è¯·æ±‚
  const limiter = rateLimit({
    interval: 60 * 1000, // 1åˆ†é’Ÿ
    uniqueTokenPerInterval: 500,
  });
  
  try {
    await limiter.check(10, request.ip);
  } catch {
    return NextResponse.json(
      { error: 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•' },
      { status: 429 }
    );
  }
  
  // ... å¤„ç†èŠå¤©è¯·æ±‚
}
```

### 3. è¾“å…¥éªŒè¯
```typescript
// é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé˜²æ­¢æ»¥ç”¨
if (message.length > 500) {
  return NextResponse.json(
    { error: 'æ¶ˆæ¯è¿‡é•¿ï¼Œè¯·æ§åˆ¶åœ¨500å­—ä»¥å†…' },
    { status: 400 }
  );
}
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

### Vercel Analytics
```typescript
// è®°å½•LLMä½¿ç”¨æƒ…å†µ
import { track } from '@vercel/analytics';

track('llm_request', {
  model: 'llama-3.2-3b',
  tokens: responseTokens,
  latency: responseTime,
});
```

### é”™è¯¯è¿½è¸ª
```typescript
// Sentry é›†æˆ
import * as Sentry from '@sentry/nextjs';

try {
  const response = await callGroq(messages);
} catch (error) {
  Sentry.captureException(error, {
    tags: {
      service: 'groq',
      model: 'llama-3.2-3b',
    }
  });
  throw error;
}
```

---

## ğŸ¯ æ¨èé…ç½®ï¼ˆå¼€ç®±å³ç”¨ï¼‰

### å°å‹ä¸ªäººé¡¹ç›®
```env
# ä½¿ç”¨Groqå…è´¹ç‰ˆ
GROQ_API_KEY=gsk_xxxxx
GROQ_MODEL=llama-3.2-3b-preview
```

### ä¸­å‹å•†ä¸šé¡¹ç›®
```env
# Groq + Togetheræ··åˆ
GROQ_API_KEY=gsk_xxxxx
GROQ_MODEL=llama-3.2-3b-preview

TOGETHER_API_KEY=xxxxx
TOGETHER_MODEL=meta-llama/Llama-3.2-3B-Instruct-Turbo
```

### å¤§å‹ä¼ä¸šé¡¹ç›®
```env
# è‡ªå»ºOllamaæœåŠ¡å™¨
OLLAMA_BASE_URL=https://ollama.yourcompany.com
OLLAMA_MODEL=llama3.2:3b

# å¤‡ç”¨äº‘ç«¯API
GROQ_API_KEY=gsk_xxxxx
```

---

## ğŸš€ å¿«é€Ÿå®æ–½

æˆ‘ç°åœ¨å°±å¸®ä½ æ›´æ–°ä»£ç ï¼Œæ”¯æŒäº‘ç«¯éƒ¨ç½²ï¼

**æ­¥éª¤**ï¼š
1. æ›´æ–° `src/lib/llm-service.ts` æ·»åŠ Groqæ”¯æŒ
2. æ³¨å†ŒGroqè´¦å·è·å–API Key
3. é…ç½®ç¯å¢ƒå˜é‡
4. éƒ¨ç½²åˆ°Vercel

éœ€è¦æˆ‘å¸®ä½ å®Œæˆè¿™äº›ä¿®æ”¹å—ï¼Ÿ ğŸ˜Š

